###  Documentation and guidance on using the Perplexity API


##  API reference for perplexity

Perplexity API
Chat Completions
Generates a model’s response for the given chat conversation.

POST
/
chat
/
completions

Try it
Authorizations
​
Authorization
stringheaderrequired
Bearer authentication header of the form Bearer <token>, where <token> is your auth token.

Body
application/json
​
model
stringrequired
The name of the model that will complete your prompt. Refer to Supported Models to find all the models offered.

Example:
"sonar"

​
messages
object[]required
A list of messages comprising the conversation so far.


Show child attributes

Example:
[
  {
    "role": "system",
    "content": "Be precise and concise."
  },
  {
    "role": "user",
    "content": "How many stars are there in our galaxy?"
  }
]
​
max_tokens
integer
The maximum number of completion tokens returned by the API. The number of tokens requested in max_tokens plus the number of prompt tokens sent in messages must not exceed the context window token limit of model requested. If left unspecified, then the model will generate tokens until either it reaches its stop token or the end of its context window.

​
temperature
numberdefault:0.2
The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower values are more deterministic.

Required range: 0 <= x < 2
​
top_p
numberdefault:0.9
The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of the tokens with top_p probability mass. We recommend either altering top_k or top_p, but not both.

Required range: 0 <= x <= 1
​
search_domain_filter
any[]
Given a list of domains, limit the citations used by the online model to URLs from the specified domains. Currently limited to only 3 domains for whitelisting and blacklisting. For blacklisting add a - to the beginning of the domain string. Only available in certain tiers - refer to our usage tiers here.

​
return_images
booleandefault:false
Determines whether or not a request to an online model should return images. Only available in certain tiers - refer to our usage tiers here.

​
return_related_questions
booleandefault:false
Determines whether or not a request to an online model should return related questions.Only available in certain tiers - refer to our usage tiers here.

​
search_recency_filter
string
Returns search results within the specified time interval - does not apply to images. Values include month, week, day, hour.

​
top_k
numberdefault:0
The number of tokens to keep for highest top-k filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, top-k filtering is disabled. We recommend either altering top_k or top_p, but not both.

Required range: 0 <= x <= 2048
​
stream
booleandefault:false
Determines whether or not to incrementally stream the response with server-sent events with content-type: text/event-stream.

​
presence_penalty
numberdefault:0
A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Incompatible with frequency_penalty.

Required range: -2 <= x <= 2
​
frequency_penalty
numberdefault:1
A multiplicative penalty greater than 0. Values greater than 1.0 penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. A value of 1.0 means no penalty. Incompatible with presence_penalty.

Required range: x > 0
​
response_format
object
Enable structured outputs with a JSON or Regex schema. Refer to the guide here for more information on how to use this parameter. Only available in certain tiers - refer to our usage tiers here.

Response
200

200
application/json

application/json
OK
​
id
string
An ID generated uniquely for each response.

​
model
string
The model used to generate the response.

Example:
"sonar"

​
object
string
The object type, which always equals chat.completion.

Example:
"chat.completion"

​
created
integer
The Unix timestamp (in seconds) of when the completion was created.

Example:
1724369245

​
citations
any[]
Citations for the generated answer.

Example:
[
  "https://www.astronomy.com/science/astro-for-kids-how-many-stars-are-there-in-space/",
  "https://www.esa.int/Science_Exploration/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe",
  "https://www.space.com/25959-how-many-stars-are-in-the-milky-way.html",
  "https://www.space.com/26078-how-many-stars-are-there.html",
  "https://en.wikipedia.org/wiki/Milky_Way"
]
​
choices
object[]
The list of completion choices the model generated for the input prompt.


Show child attributes

Example:
[
  {
    "index": 0,
    "finish_reason": "stop",
    "message": {
      "role": "assistant",
      "content": "The number of stars in the Milky Way galaxy is estimated to be between 100 billion and 400 billion stars. The most recent estimates from the Gaia mission suggest that there are approximately 100 to 400 billion stars in the Milky Way, with significant uncertainties remaining due to the difficulty in detecting faint red dwarfs and brown dwarfs."
    },
    "delta": { "role": "assistant", "content": "" }
  }
]
​
usage
object
Usage statistics for the completion request.


Hide child attributes

​
usage.prompt_tokens
integer
The number of tokens provided in the request prompt.

​
usage.completion_tokens
integer
The number of tokens generated in the response output.

​
usage.total_tokens
integer
The total number of tokens used in the chat completion (prompt + completion).

twitter
linkedin
discord
website

cURL

Python

JavaScript

PHP

Go

Java

Copy
import requests

url = "https://api.perplexity.ai/chat/completions"

payload = {
    "model": "sonar",
    "messages": [
        {
            "role": "system",
            "content": "Be precise and concise."
        },
        {
            "role": "user",
            "content": "How many stars are there in our galaxy?"
        }
    ],
    "max_tokens": 123,
    "temperature": 0.2,
    "top_p": 0.9,
    "search_domain_filter": None,
    "return_images": False,
    "return_related_questions": False,
    "search_recency_filter": "<string>",
    "top_k": 0,
    "stream": False,
    "presence_penalty": 0,
    "frequency_penalty": 1,
    "response_format": None
}
headers = {
    "Authorization": "Bearer <token>",
    "Content-Type": "application/json"
}

response = requests.request("POST", url, json=payload, headers=headers)

print(response.text)

200

400

401

429

500

504

524

Copy
{
  "id": "3c90c3cc-0d44-4b50-8888-8dd25736052a",
  "model": "sonar",
  "object": "chat.completion",
  "created": 1724369245,
  "citations": [
    "https://www.astronomy.com/science/astro-for-kids-how-many-stars-are-there-in-space/",
    "https://www.esa.int/Science_Exploration/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe",
    "https://www.space.com/25959-how-many-stars-are-in-the-milky-way.html",
    "https://www.space.com/26078-how-many-stars-are-there.html",
    "https://en.wikipedia.org/wiki/Milky_Way"
  ],
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "The number of stars in the Milky Way galaxy is estimated to be between 100 billion and 400 billion stars. The most recent estimates from the Gaia mission suggest that there are approximately 100 to 400 billion stars in the Milky Way, with significant uncertainties remaining due to the difficulty in detecting faint red dwarfs and brown dwarfs."
      },
      "delta": {
        "role": "assistant",
        "content": ""
      }
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 70,
    "total_tokens": 84
  }
}


##  Supported Models
Model	Context Length	Model Type
sonar-deep-research	128k	Chat Completion
sonar-reasoning-pro	128k	Chat Completion
sonar-reasoning	128k	Chat Completion
sonar-pro	200k	Chat Completion
sonar	128k	Chat Completion
r1-1776	128k	Chat Completion
sonar-reasoning-pro and sonar-pro have a max output token limit of 8k.
The reasoning models output CoTs in their responses as well.
r1-1776 is an offline chat model that does not use our search subsystem.
​
Model Descriptions
​
sonar-deep-research
Deep Research conducts comprehensive, expert-level research and synthesizes it into accessible, actionable reports.

Sample Prompt:

How have this year’s US tariffs affected commodity prices around the world? Give me a detailed comprehensive timeline.

​
Features:
Exhaustive Research: Performs dozens of searches, reading hundreds of sources.
Expert-level Analysis: Reasons autonomously and generates detailed insights across a full range of subjects—finance, marketing, technology, health, travel, and beyond.
Report Generation: Once the source materials have been fully evaluated, the agent synthesizes all the research into a clear and comprehensive report.
Data Privacy: No training on customer data.
Scale Pricing.
​
sonar-reasoning-pro
Premier reasoning offering powered by DeepSeek R1 with Chain of Thought (CoT).

Sample Prompt:

Make an investment thesis for upcoming US IPOs.

​
sonar-pro
Premier search offering with search grounding, supporting advanced queries and follow-ups.

Sample Query:

What is [company’s] projected Q4 strategy?

​
sonar
Lightweight offering with search grounding, quicker and cheaper than Sonar Pro.

Sample Query:

Where is [company] based?

​
r1-1776
R1-1776 is a version of the DeepSeek R1 model that has been post-trained to provide uncensored, unbiased, and factual information.